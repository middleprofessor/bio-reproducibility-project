---
title: Abandon ANOVA-type experiments
author: "Jeff Walker"
date: "April 13, 2018"
output:
  word_document: default
  html_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Today's post was motivated by a tweet to Kirk et al.'s [Empirical evidence that metabolic theory describes the temperature dependency of within-host parasite dynamics](http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.2004608){target="_blank"}. Here is an image of the infected host, provided by first author Devin Kirk, a PhD Candidate in the Department of Ecology and Evolutionary Biology at the University of Toronto, and taken by a former student in the lab, Jessica Phillips.

```{r image, out.width = "400px", echo=FALSE}
image_path <- "../images/Gut_parasite_labelled.JPG"
knitr::include_graphics(image_path)
```

Why this paper motivated this post requires a wee bit of background. Biology departments are doing fewer canned, demonstrate-a-concept labs and more inquiry-based learning in upper level and even introductory biology labs. Often these are multi-day lab experiences that take the form of students designing and conducting simple experiments. I suspect that the reports generated by the students in these labs will mostly include bar plots to show the experimental results. This early training will be hard to de-train later, which motivated me to write [part I](https://rapidecology.com/2018/04/09/when-do-we-introduce-best-statistical-practices-to-undergraduate-biology-majors/){target="_blank"} of this series, where I introduced the [Harrell plot](https://www.middleprofessor.com/files/quasipubs/harrell_plot_intro.html){target="_blank"} as an alternative to bar plots.

Here I'm going to suggest something more radical -- that we abandon^[I am intentionally provoking discussion] using simple experiments like this to introduce the process of science to students.

Wait, what?

I'd like to propose that simple "comparison of means" experiments (those analyzed with a t-test or ANOVA) train our brains to think that this is the goal of science -- to discover **if** an effect exists. My evidence for this is the literature in ecology or physiology, or cell biology, a large part of which is absurdly concerned with "tests" "for" the "presence" of effects. But the goal of our science should be mechanistic and predictive models that make quantitative predictions that might be tested with experiments. Or maybe the experiments are used to generate the parameters for the models. A Harrell plot is a partial answer to this -- it nudges students and researchers to think about effect sizes and not $p$-values, or "asterisks and letters". But a Harrell plot is best used for simple experiments with only one or a few levels per factor (and only one or two factors).

The absurdity of the *t*-test or $2 \times 2$ ANOVA way of doing science is apparent if something like temperature and CO2 are the experimental factors -- for example in the many global climate change studies. What in ecology or physiology or cell biology is not related to temperature and CO2?

[Kirk et al.](http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.2004608){target="_blank"} motivated this post because they report the results of a regression design used to measure experimentally the shape of the temperature effect on host survival and parasite burden over a broad temperature range. They then fit mechanistic (metabolic theory of ecology) models of host-parasite dynamics to the data to estimate model parameters. This paper in particular motivated this post because I happened to see it in my twitter feed while thinking about regression designs as a "better practice" for research than traditional ANOVA-type designs.

And if its better practice for research, its better practice for inquiry-based experiences in undergraduate labs from the *start* -- Introductory biology labs. Again, it's much harder to de-train fuzzy thinking than to train it right the first time.



